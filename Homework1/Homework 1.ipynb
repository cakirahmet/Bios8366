{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer all questions and submit them either as an IPython notebook, LaTeX document, or Markdown document. Each question is worth 25 points.\n",
    "\n",
    "This homework is due Friday, October 9, 2015."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "The data below provides counts of a flour beetle (Tribolium confusum) population at various points in time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "days = 0,8,28,41,63,79,97,117,135,154\n",
    "beetles = 2,47,192,256,768,896,1120,896,1184,1024\n",
    "\n",
    "plt.plot(days, beetles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An elementary model for population growth is the logistic model:\n",
    "\n",
    "$$\\frac{dN}{dt} = rN\\left(1 - \\frac{N}{K}\\right)$$\n",
    "\n",
    "where $N$ is population size, $t$ is time, $r$ is a growth rate parameter, and $K$ is a parameter that represents the population carrying capacity of the environment. The solution to this differential equation is given by: \n",
    "\n",
    "$$N_t = f(t) = \\frac{KN_0}{N_0 + (K - N_0)\\exp(-rt)}$$\n",
    "\n",
    "where $N_t$ denotes the population size at time $t$. \n",
    "\n",
    "1) Fit the logistic growth model to the flour beetle data using optimization to minimize the sum of squared errors between model predictions and observed counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "import math\n",
    "def calcError(args):\n",
    "    error = 0;\n",
    "    for i in range(len(days)):\n",
    "        calcBeetles = f([args[0],beetles[0],args[1],days[i]])\n",
    "        error += (calcBeetles-beetles[i])**2\n",
    "    return error\n",
    "\n",
    "def f(args):\n",
    "    #K N0 r t\n",
    "    calcBeetles = (args[0]*args[1])/(args[1]+(args[0]-args[1])*math.exp(-args[2]*args[3]))\n",
    "    return calcBeetles\n",
    "\n",
    "\n",
    "result = minimize(calcError, [1700,0.4],method = 'Nelder-Mead')\n",
    "K = result.x[0]\n",
    "r = result.x[1]\n",
    "calcBeetles = np.zeros([len(days),1])\n",
    "for i in range(len(days)):\n",
    "    calcBeetles[i] = f([K,beetles[0],r,days[i]])\n",
    "\n",
    "print(result)\n",
    "plt.plot(days,calcBeetles)\n",
    "plt.plot(days,beetles,'ro')\n",
    "print(calcBeetles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) In many population modeling applications, an assumption of lognormality is adopted. The simplest assumption would be that the $\\log(N_t)$ are independent and normally distributed with mean $\\log[f(t)]$ and variance $\\sigma^2$. Find the MLEs under this assumption, and provide estimates of standard errors and correlation between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "days_new = np.copy(days)\n",
    "beetles_new = np.copy(beetles)\n",
    "\n",
    "def calcLikelihood(args):\n",
    "    likelyhood = 0\n",
    "    for i in range(len(beetles_new)):\n",
    "        logft = f([args[0],beetles_new[0],args[1],days_new[i]])\n",
    "        likelyhood += (-1*norm.logpdf(np.log(beetles_new[i]),loc=logft,scale=args[2]))\n",
    "    return likelyhood\n",
    "\n",
    "def f(args):\n",
    "    #K N0 r t\n",
    "    try:\n",
    "        calcBeetles = np.log((args[0]*args[1])/(args[1]+(args[0]-args[1])*np.exp(-args[2]*args[3])))\n",
    "    except OverflowError:\n",
    "        print(\"Error\")\n",
    "    return calcBeetles\n",
    "\n",
    "logBeetles_new = np.zeros([len(beetles),1])\n",
    "\n",
    "nTimes =  10\n",
    "K = np.zeros(nTimes)\n",
    "r = np.zeros(nTimes)\n",
    "var = np.zeros(nTimes)\n",
    "length = len(beetles_new)\n",
    "counter = 0\n",
    "for i in range(nTimes):\n",
    "    index = np.random.randint(0,length-1,length)\n",
    "    for t in range(length):\n",
    "        beetles_new[t] = np.copy(beetles[index[t]])\n",
    "        days_new[t] = np.copy(days[index[t]])\n",
    "    for t in range(len(beetles_new)):\n",
    "        logBeetles_new[t] = math.log(beetles_new[t])\n",
    "    var0 = np.var(logBeetles_new)\n",
    "    result = minimize(calcLikelihood, [1700,0.4,var0],method = 'Nelder-Mead',options = {'xtol':1e-8, 'disp' : False})\n",
    "    #print(result)\n",
    "    if result.success:\n",
    "        print('New K',result.x[0],'new r',result.x[1],'New var',result.x[2])\n",
    "        K[counter] = result.x[0]\n",
    "        r[counter] = result.x[1]\n",
    "        var[counter] = result.x[2]\n",
    "        counter+=1\n",
    "K_final = np.delete(K,range(counter,len(K)))\n",
    "r_final = np.delete(r,range(counter,len(K)))\n",
    "var_final = np.delete(var,range(counter,len(K)))\n",
    "\n",
    "\n",
    "print('variance of K',np.var(K_final))\n",
    "print('variance of r', np.var(r_final))\n",
    "print('variance of variance',np.var(var_final))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "1)Implement simulated annealing for minimizing the AIC for the baseball salary regression problem. Model your algorithm on the example given in class. \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aic = lambda g: g.nobs * np.log((g.resid**2).sum()/g.nobs) + 2*len(g.beta)\n",
    "\n",
    "def plotResults(aic_values,aic_best,solution_best):\n",
    "\n",
    "    plt.plot(aic_values)\n",
    "    plt.xlim(0, len(aic_values))\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('AIC')\n",
    "    print('Best AIC: {0}\\nBest solution: {1}\\nDiscovered at iteration {2}'.format(aic_best,\n",
    "                np.where(solution_best==True),\n",
    "                np.where(aic_values==aic_best)[0][0]))\n",
    "    plt.plot(np.where(aic_values==aic_best)[0][0], aic_best, 'ro')\n",
    "    plt.show()\n",
    "\n",
    "def simAnnealing(periods,cooling,tau_start,numChangeNeigh):\n",
    "    baseball = pd.read_table(\"/Users/Ahmet/Box Sync/Classes/Vanderbilt/AdvancedStatisticalComputing/Bios8366/data/textbook/baseball.dat\", sep='\\s+')\n",
    "    predictors = baseball.copy()\n",
    "    logsalary = predictors.pop('salary').apply(np.log)\n",
    "\n",
    "    nrows, ncols = predictors.shape\n",
    "\n",
    "    aic_values = []\n",
    "    solution_current = solution_best = np.random.binomial(1, 0.5, ncols).astype(bool)\n",
    "    solution_vars = predictors[predictors.columns[solution_current]]\n",
    "    g = pd.ols(y=logsalary, x=solution_vars)\n",
    "    aic_best = aic(g)\n",
    "    aic_values.append(aic_best)\n",
    "\n",
    "    # Cooling schedule\n",
    "    tau = [tau_start * 0.9**i for i in range(periods)]\n",
    "    for j in range(periods):\n",
    "\n",
    "        for i in range(cooling[j]):\n",
    "\n",
    "            # Random change n-neighborhood\n",
    "            flip = np.random.choice(ncols,numChangeNeigh,replace = False)\n",
    "            for i in range(numChangeNeigh):\n",
    "                solution_current[flip[i]] = not solution_current[flip[i]]\n",
    "            solution_vars = predictors[predictors.columns[solution_current]]\n",
    "            g = pd.ols(y=logsalary, x=solution_vars)\n",
    "            aic_step = aic(g)\n",
    "            alpha = min(1, np.exp((aic_values[-1] - aic_step)/tau[j]))\n",
    "\n",
    "            if ((aic_step < aic_values[-1]) or (np.random.uniform() < alpha)):\n",
    "                # Accept proposed solution\n",
    "                aic_values.append(aic_step)\n",
    "                if aic_step < aic_best:\n",
    "                    # Replace previous best with this one\n",
    "                    aic_best = aic_step\n",
    "                    solution_best = solution_current.copy()\n",
    "            else:\n",
    "                # Revert solution\n",
    "                for i in range(numChangeNeigh):\n",
    "                    solution_current[flip[i]] = not solution_current[flip[i]]\n",
    "                aic_values.append(aic_values[-1])\n",
    "    return aic_values,aic_best,solution_best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    a) Compare the effects of different cooling schedules (different temperatures and different durations at each temperature)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Spend more time at the low temperatures and start from a low temperature\n",
    "periods = 15\n",
    "numChangeNeigh = 1\n",
    "#cooling\n",
    "cooling_1 = [20* i for i in range(1,periods+1)] #[20 40 80 ... 16*20]\n",
    "tau_start = 5\n",
    "aic_values_1,aic_best_1,solution_best_1 = simAnnealing(periods,cooling_1,tau_start,numChangeNeigh)\n",
    "plotResults(aic_values_1,aic_best_1,solution_best_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Spend much more time at the high temperature and start from high temperature\n",
    "cooling_2 = np.fliplr([cooling_1])[0]\n",
    "# inverse of cooling_1\n",
    "tau_start = 80\n",
    "aic_values_2,aic_best_2,solution_best_2 = simAnnealing(periods,cooling_2,tau_start,numChangeNeigh)\n",
    "plotResults(aic_values_2,aic_best_2,solution_best_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Spend equal amount of time but start from medium temperature\n",
    "cooling_3 = [120]*int(periods/3) + [120]*int(periods/3) + [120]*int(periods/3) #\n",
    "tau_start = 50\n",
    "aic_values_3,aic_best_3,solution_best_3 = simAnnealing(periods,cooling_3,tau_start,numChangeNeigh)\n",
    "plotResults(aic_values_3,aic_best_3,solution_best_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ossicilations of the aic_values increase as the algorithm spend more time on high temperatures and start from a relatively higher temperature. This behaviour is expected since higher temperature values increase the chances of accepting a value even if it is not the best so far. When we use a lower temperature, on the other hand, and spent more time at the lower temperatures, we see a much flatter-pattern where the aic_value does not change as much. The minimum valeu of the aic does not change a lot between different cooling scheduels.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Compare the effect of a proposal distribution that is discrete uniform over 2-neighborhoods versus one that is discrete uniform over 3-neighborhoods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Over 2-neighborhoods\n",
    "numChangeNeigh = 2\n",
    "tau_start = 20\n",
    "aic_values_1,aic_best_1,solution_best_1 = simAnnealing(periods,cooling_1,tau_start,numChangeNeigh)\n",
    "plotResults(aic_values_1,aic_best_1,solution_best_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Over 3-neighborhoods\n",
    "numChangeNeigh = 3\n",
    "aic_values_1,aic_best_1,solution_best_1 = simAnnealing(periods,cooling_1,tau_start,numChangeNeigh)\n",
    "plotResults(aic_values_1,aic_best_1,solution_best_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the plots above, the using uniform 3 neighborhoods create bigger oscillation between aic_values than 2 neighborhoods. There is not that much differnce in between besides how much the aic valus change.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Implement a genetic algorithm for minimizing the AIC for the baseball salary regression problem. Model your algorithm on Example 3.5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_fitness(aic_values):\n",
    "    P = len(aic_values)\n",
    "    aic_rank = (-aic_values).argsort().argsort()+1.\n",
    "    return 2.*aic_rank/(P*(P+1.))\n",
    "\n",
    "def geneticAlgo(mutation_rate,pop_size,useFitnessDad):\n",
    "\n",
    "    baseball = pd.read_table(\"/Users/Ahmet/Box Sync/Classes/Vanderbilt/AdvancedStatisticalComputing/Bios8366/data/textbook/baseball.dat\", sep='\\s+')\n",
    "    predictors = baseball.copy()\n",
    "    logsalary = predictors.pop('salary').apply(np.log)\n",
    "    nrows, ncols = predictors.shape\n",
    "    iterations = 100\n",
    "\n",
    "    aic_best = []\n",
    "    best_solution = []\n",
    "    aic_history = []\n",
    "\n",
    "    # Initialize genotype\n",
    "    current_gen = np.random.binomial(1, 0.5, pop_size*ncols).reshape((pop_size, ncols))\n",
    "\n",
    "\n",
    "    for i in range(iterations):\n",
    "\n",
    "        # Get phenotype\n",
    "        current_phe = [predictors[predictors.columns[g.astype(bool)]] for g in current_gen]\n",
    "        # Calculate AIC\n",
    "        current_aic = np.array([aic(pd.ols(y=logsalary, x=x)) for x in current_phe])\n",
    "        # Get lowest AIC\n",
    "        aic_best.append(current_aic[np.argmin(current_aic)])\n",
    "        best_solution.append(current_gen[np.argmin(current_aic)])\n",
    "\n",
    "        # Calculate fitness according to AIC rank\n",
    "        fitness = calculate_fitness(current_aic)\n",
    "\n",
    "        # Choose first parents according to fitness\n",
    "        moms = np.random.choice(range(pop_size), size=int(pop_size/2), p=fitness)\n",
    "        # Choose second parents randomly\n",
    "        if useFitnessDad:\n",
    "            dads = np.random.choice(range(pop_size), size=int(pop_size/2),p = fitness)\n",
    "        else:\n",
    "            dads = np.random.choice(range(pop_size), size=int(pop_size/2))\n",
    "                \n",
    "\n",
    "        next_gen = []\n",
    "        for x,y in zip(current_gen[moms], current_gen[dads]):\n",
    "            # Crossover\n",
    "            cross = np.random.randint(0, ncols)\n",
    "            child1 = np.r_[x[:cross], y[cross:]]\n",
    "            child2 = np.r_[y[:cross], x[cross:]]\n",
    "            # Mutate\n",
    "            m1 = np.random.binomial(1, mutation_rate, size=ncols).astype(bool)\n",
    "            child1[m1] = abs(child1[m1]-1)\n",
    "            m2 = np.random.binomial(1, mutation_rate, size=ncols)\n",
    "            child2[m2] = abs(child1[m2]-1)\n",
    "            next_gen += [child1, child2]\n",
    "\n",
    "        # Increment generation\n",
    "        current_gen = np.array(next_gen)\n",
    "        # Store AIC values\n",
    "        aic_history.append(current_aic)\n",
    "    return aic_best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 1. Compare the effects of using different mutation rates.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Mutation rate = 0.001, pop size = 30, dont use fitness for dad\n",
    "aic_best = geneticAlgo(0.001,30,0)\n",
    "plt.plot(aic_best,color='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Mutation rate = 0.1, pop size = 30, dont use fitness for dad\n",
    "aic_best = geneticAlgo(0.1,30,0)\n",
    "plt.plot(aic_best,color='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing the mutation rate causes the aic value to change more drastically as the algorithm iterates. This behaviour is expected since mutation rate basically increses the change in the genetic code that will help the algorithm to explore more possible genetic codes. It will also increases the chances of moving away from finding the best aic value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 2. Compare the effects of using different generation sizes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Mutation rate = 0.01, pop size = 10, dont use fitness for dad\n",
    "aic_best = geneticAlgo(0.01,10,0)\n",
    "plt.plot(aic_best,color='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Mutation rate = 0.01, pop size = 100, dont use fitness for dad\n",
    "aic_best = geneticAlgo(0.01,100,0)\n",
    "plt.plot(aic_best,color='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can say that the effect of increasing the population size is similiar to increasing mutation rate.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Instead of the selection mechanism used in the class example, try using independent selection of both parents with probabilities proportional to their fitness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Mutation rate = 0.01, pop size = 20,  use fitness for dad\n",
    "aic_best = geneticAlgo(0.01,20,1)\n",
    "plt.plot(aic_best,color='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using fitness decreases exploration of possible solutions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "\n",
    "Use the combinatorial optimization method of your choice to obtain a solution to the traveling salesman problem for the Brazilian cities described in the lecture notes, using minimum total distance as the criterion. Use the the first city listed in the dataset as \"home\" (*i.e.* the trip must start and end there. I will award 5 bonus points to the best solution!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import math\n",
    "\n",
    "def parse_latlon(x):\n",
    "    d, m, s = map(float, x.split(':'))\n",
    "    ms = m/60. + s/3600.\n",
    "    if d<0:\n",
    "        return d - ms\n",
    "    return d + ms\n",
    "\n",
    "def swapCities(cities, nSwap):\n",
    "    if nSwap % 2 != 0:\n",
    "        nSwap -= 1\n",
    "    nSwap = int(max(nSwap, 2))\n",
    "    nSwap = int(nSwap / 2)\n",
    "    citiesToSwap = np.random.choice(len(cities) - 2,nSwap*2,replace=False)\n",
    "    while citiesToSwap[0] == 0 or citiesToSwap[1] == 0:\n",
    "        citiesToSwap = np.random.choice(len(cities) - 1,nSwap*2,replace=False)\n",
    "    for i in range(nSwap):\n",
    "        tempCity = np.copy(cities[citiesToSwap[i*2]])\n",
    "        cities[citiesToSwap[i*2]] = cities[citiesToSwap[i*2+1]]\n",
    "        cities[citiesToSwap[i*2+1]] = tempCity\n",
    "\n",
    "    return cities\n",
    "\n",
    "def calculateTotalDistance(cities):\n",
    "    dist = 0\n",
    "    for i in range(len(cities) - 1):\n",
    "        dist += math.sqrt((cities[i][1] - cities[i + 1][1]) ** 2 + (cities[i][2] - cities[i + 1][2]) ** 2)\n",
    "    return dist\n",
    "\n",
    "\n",
    "cities =  pd.read_csv('../data/brasil_capitals.txt', \n",
    "                      names=['city','lat','lon'])[['lat','lon']].applymap(parse_latlon)\n",
    "\n",
    "beginTemp = 10\n",
    "endTemp = 0.001\n",
    "coolingFactor = 0.9\n",
    "period = 840\n",
    "bestDistances = []\n",
    "cityArray = np.transpose(np.array((range(len(cities)), cities['lat'], cities['lon']), np.float64))\n",
    "numSwap = 2\n",
    "bestCityOrder = np.random.permutation(cityArray)\n",
    "bestCityOrder = np.vstack([bestCityOrder,bestCityOrder[0]])\n",
    "bestDistance = calculateTotalDistance(bestCityOrder)\n",
    "bestDistances.append(bestDistance)\n",
    "\n",
    "for j in range(period):\n",
    "\n",
    "    temperature = beginTemp\n",
    "\n",
    "    currentDistance = bestDistance\n",
    "    currentCities = np.copy(bestCityOrder)\n",
    "\n",
    "    newCities = np.copy(currentCities)\n",
    "    newDistance = currentDistance\n",
    "\n",
    "\n",
    "\n",
    "    while True:\n",
    "        newCities = swapCities(newCities, numSwap)\n",
    "        newDistance = calculateTotalDistance(newCities)\n",
    "\n",
    "        if newDistance < currentDistance or math.exp(\n",
    "                        (currentDistance - newDistance) / temperature) > np.random.random():\n",
    "            currentDistance = newDistance\n",
    "            currentCities = np.copy(newCities)\n",
    "        else:\n",
    "            newDistance = currentDistance\n",
    "            newCities = currentCities[:]\n",
    "        if newDistance < bestDistance:\n",
    "            bestCityOrder = deepcopy(newCities)\n",
    "            bestDistance = newDistance\n",
    "            bestDistances.append(bestDistance)\n",
    "        temperature *= coolingFactor\n",
    "        #numSwap = int(numSwap * coolingFactor)\n",
    "        if temperature < endTemp:\n",
    "            break\n",
    "\n",
    "#print(bestDistances)\n",
    "print('best distance = ',min(bestDistances))\n",
    "#print(bestCityOrder)\n",
    "plt.plot(bestCityOrder[:,1],bestCityOrder[:,2],'ro',bestCityOrder[:,1],bestCityOrder[:,2],'b')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "\n",
    "Suppose $y$ has a binomial distribution with parameters $n$ and $p$, and we are interested in the log-odds value $\\theta = \\log(p/(1 − p))$. Our prior for $\\theta$ is that $\\theta \\sim N(\\mu, \\sigma^2)$. It follows that the posterior density of $\\theta$ is given, up to a proportionality constant, by:\n",
    "\n",
    "$$g(\\theta | y) \\propto \\frac{\\exp(y\\theta)}{(1 + exp(\\theta))^n} \\exp\\left[\\frac{-(\\theta − \\mu)^2}{2\\sigma^2}\\right]$$\n",
    "\n",
    "For example, suppose we are interested in learning about the probability that a possibly-biased coin lands heads when tossed. *A priori* we believe that the coin is fair, so we assign $\\theta$ a $N(0,.25)$ prior. We toss the coin $n = 5$ times and obtain $y = 5$ heads.\n",
    "\n",
    "1. Using a normal approximation to the posterior density, compute the probability that the coin is biased toward heads (i.e., that θ is posi- tive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "from scipy.optimize import fmin_bfgs\n",
    "def posterior(param, n, y):\n",
    "\n",
    "    theta = param\n",
    "    post = y*theta\n",
    "    post -= n*np.log(1+np.exp(theta))\n",
    "    post -= ((theta-mu)**2)/(2*sigma)\n",
    "\n",
    "    return post\n",
    "\n",
    "def calc_diff(theta, n, y):\n",
    "\n",
    "    return posterior(theta, n, y) - np.log(norm.pdf(theta,mu,sigma))\n",
    "\n",
    "calc_diff_min = lambda *args: -calc_diff(*args)\n",
    "\n",
    "posterior_min = lambda *args: -posterior(*args)\n",
    "\n",
    "mu = 0\n",
    "sigma = 0.25\n",
    "init_value = (0.1)\n",
    "n= 5\n",
    "y = 5\n",
    "\n",
    "opt = fmin_bfgs(posterior_min, init_value,\n",
    "          args=(n, y), full_output=True)\n",
    "mode, var = opt[0], opt[3]\n",
    "\n",
    "prob = 1 - norm.cdf(0,mode,var)\n",
    "print(prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Using the prior density as a proposal density, design a rejection algo- rithm for sampling from the posterior distribution. Using simulated draws from your algorithm, approximate the probability that the coin is biased toward heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def reject(post, n, data, c):\n",
    "\n",
    "    k = len(mode)\n",
    "\n",
    "    # Draw samples from g(theta)\n",
    "    theta = norm.rvs(mu, sigma, size=n)\n",
    "\n",
    "    # Calculate probability under g(theta)\n",
    "    gvals = np.array([np.log(norm.pdf(t, mu, sigma)) for t in theta])\n",
    "\n",
    "    # Calculate probability under f(theta)\n",
    "    fvals = np.array([post(t, data[0], data[1]) for t in theta])\n",
    "\n",
    "    # Calculate acceptance probability\n",
    "    p = np.exp(fvals - gvals + c)\n",
    "\n",
    "    return theta[np.random.random(n) < p]\n",
    "\n",
    "c_init = 0.8\n",
    "opt = fmin_bfgs(calc_diff_min,c_init,\n",
    "                args=(5, 5),\n",
    "                full_output=True)\n",
    "\n",
    "print('c values = ',opt[1])\n",
    "prob = reject(posterior,1e4,[n,y],opt[1])\n",
    "\n",
    "counter = 0\n",
    "for i in range(len(prob)):\n",
    "    if prob[i] > 0:\n",
    "        counter+=1\n",
    "print('Probability of heads = ',counter/prob.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Using the prior density as a proposal density, simulate values from the posterior distribution using the SIR algorithm. Approximate the probability that the coin is biased toward heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sampSize= 10000\n",
    "\n",
    "normalSamples = norm.rvs(mu, sigma, size=sampSize)\n",
    "\n",
    "f_theta = np.array([posterior(t, n, y) for t in normalSamples])\n",
    "\n",
    "q_theta = np.array([np.log(norm.pdf(t, mu, sigma)) for t in normalSamples])\n",
    "\n",
    "w = np.exp(f_theta - q_theta - max(f_theta - q_theta))\n",
    "p_sir = w/w.sum()\n",
    "theta_sir = normalSamples[np.random.choice(range(len(normalSamples)), size=10000, p=p_sir)]\n",
    "counter = 1\n",
    "for i in range(len(theta_sir)):\n",
    "    if theta_sir[i] > 0:\n",
    "        counter+=1\n",
    "print(counter/len(theta_sir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
